{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d10149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b866e94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/09 16:15:21 WARN Utils: Your hostname, Giordano, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/09 16:15:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 16:15:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[*]')\\\n",
    "    .appName('load')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3e5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.parquet(\n",
    "    \"../../datalake/serving/train_bovespa_plano_real.parquet\"\n",
    ")\n",
    "test_df  = spark.read.parquet(\n",
    "    \"../../datalake/serving/test_bovespa_plano_real.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94596bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 16:15:29 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/09 16:15:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/09 16:15:50 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:15:50 WARN BlockManager: Persisting block rdd_213_3 to disk instead.\n",
      "25/11/09 16:15:50 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:15:50 WARN BlockManager: Persisting block rdd_213_0 to disk instead.\n",
      "25/11/09 16:15:50 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:15:50 WARN BlockManager: Persisting block rdd_213_4 to disk instead.\n",
      "25/11/09 16:15:50 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:15:50 WARN BlockManager: Persisting block rdd_213_5 to disk instead.\n",
      "25/11/09 16:15:50 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:15:50 WARN BlockManager: Persisting block rdd_213_2 to disk instead.\n",
      "25/11/09 16:15:50 WARN MemoryStore: Not enough space to cache rdd_213_1 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:15:50 WARN BlockManager: Persisting block rdd_213_1 to disk instead.\n",
      "25/11/09 16:15:57 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 113.4 MiB so far)\n",
      "25/11/09 16:15:57 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:15:58 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 6.2 MiB so far)\n",
      "25/11/09 16:15:58 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 264.4 MiB so far)\n",
      "25/11/09 16:15:58 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 6.2 MiB so far)\n",
      "25/11/09 16:15:58 WARN MemoryStore: Not enough space to cache rdd_213_1 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:16:06 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:16:06 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:06 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:06 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:06 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:06 WARN MemoryStore: Not enough space to cache rdd_213_1 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:16 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:16 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:16 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:16 WARN MemoryStore: Not enough space to cache rdd_213_1 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:16 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:16:16 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:28 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:28 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:28 WARN MemoryStore: Not enough space to cache rdd_213_1 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:28 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:28 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:16:28 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:40 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:40 WARN MemoryStore: Not enough space to cache rdd_213_1 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:40 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:40 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:40 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 49.8 MiB so far)\n",
      "25/11/09 16:16:41 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 74.7 MiB so far)\n",
      "25/11/09 16:16:56 ERROR Executor: Exception in task 0.0 in stage 98.0 (TID 409) \n",
      "org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Labels MUST be in {0, 1}, but got 2.0 SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_0_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/09 16:16:56 WARN TaskSetManager: Lost task 0.0 in stage 98.0 (TID 409) (10.255.255.254 executor driver): org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Labels MUST be in {0, 1}, but got 2.0 SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_0_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/11/09 16:16:56 ERROR TaskSetManager: Task 0 in stage 98.0 failed 1 times; aborting job\n",
      "25/11/09 16:16:56 ERROR Instrumentation: org.apache.spark.SparkRuntimeException: [USER_RAISED_EXCEPTION] Labels MUST be in {0, 1}, but got 2.0 SQLSTATE: P0001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.raiseError(QueryExecutionErrors.scala:2782)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.raiseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.caseWhen_0_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:1245)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1500)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1500)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1473)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:334)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:62)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n"
     ]
    },
    {
     "ename": "SparkRuntimeException",
     "evalue": "[USER_RAISED_EXCEPTION] Labels MUST be in {0, 1}, but got 2.0 SQLSTATE: P0001",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSparkRuntimeException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m lr_model = lr.fit(train_df)\n\u001b[32m      7\u001b[39m rf_model = rf.fit(train_df)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m gbt_model = \u001b[43mgbt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m lr_preds = lr_model.transform(test_df)\n\u001b[32m     10\u001b[39m rf_preds = rf_model.transform(test_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tde-analytics-b3/.venv/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tde-analytics-b3/.venv/lib/python3.11/site-packages/pyspark/ml/util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tde-analytics-b3/.venv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tde-analytics-b3/.venv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tde-analytics-b3/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/tde-analytics-b3/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mSparkRuntimeException\u001b[39m: [USER_RAISED_EXCEPTION] Labels MUST be in {0, 1}, but got 2.0 SQLSTATE: P0001"
     ]
    }
   ],
   "source": [
    "train_df = train_df.withColumnRenamed(\"Market_Type_idx\", \"label\")\n",
    "test_df = test_df.withColumnRenamed(\"Market_Type_idx\", \"label\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=10)\n",
    "\n",
    "lr_model = lr.fit(train_df)\n",
    "rf_model = rf.fit(train_df)\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "lr_preds = lr_model.transform(test_df)\n",
    "rf_preds = rf_model.transform(test_df)\n",
    "dt_preds = dt_model.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "results = {\n",
    "    \"LogisticRegression\": evaluator.evaluate(lr_preds),\n",
    "    \"RandomForest\": evaluator.evaluate(rf_preds),\n",
    "    \"GBT\": evaluator.evaluate(dt_preds)\n",
    "}\n",
    "\n",
    "for model, acc in results.items():\n",
    "    print(f\"{model} Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.write().overwrite().save(\"./models/bovespa_lr\")\n",
    "rf_model.write().overwrite().save(\"./models/bovespa_rf\")\n",
    "dt_model.write().overwrite().save(\"./models/bovespa_gbt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
